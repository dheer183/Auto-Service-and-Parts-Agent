{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJXVns7aLKi9"
      },
      "source": [
        "In order to run this, you should have vector database `(vector_db_)` already stored as .zip. You can download it using the link\n",
        "https://drive.google.com/drive/folders/1o2Y-BLNWNFT5zesysd2n3QZPtN0Jro3I?usp=drive_link"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "HAM-iCpld5j8",
        "outputId": "2c84211f-8176-4ae0-8273-b93f09f23cd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb==0.5.5\n",
            "  Downloading chromadb-0.5.5-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting langchain-chroma==0.1.2\n",
            "  Downloading langchain_chroma-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting langchain==0.2.11\n",
            "  Downloading langchain-0.2.11-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-community==0.2.10\n",
            "  Downloading langchain_community-0.2.10-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting langchain-text-splitters==0.2.2\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting langchain-groq==0.1.6\n",
            "  Downloading langchain_groq-0.1.6-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting transformers==4.43.2\n",
            "  Downloading transformers-4.43.2-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence-transformers==3.0.1\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting unstructured==0.15.0\n",
            "  Downloading unstructured-0.15.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting build>=1.0.3 (from chromadb==0.5.5)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.5.5) (2.9.2)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb==0.5.5)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi>=0.95.2 (from chromadb==0.5.5)\n",
            "  Downloading fastapi-0.115.5-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb==0.5.5)\n",
            "  Downloading uvicorn-0.32.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.5.5) (1.26.4)\n",
            "Collecting posthog>=2.4.0 (from chromadb==0.5.5)\n",
            "  Downloading posthog-3.7.3-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.5.5) (4.12.2)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb==0.5.5)\n",
            "  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.5.5) (1.28.2)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb==0.5.5)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb==0.5.5)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.49b2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.5.5) (1.28.2)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.5.5) (0.20.3)\n",
            "Collecting pypika>=0.48.9 (from chromadb==0.5.5)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.5.5) (4.66.6)\n",
            "Collecting overrides>=7.3.1 (from chromadb==0.5.5)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb==0.5.5) (6.4.5)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.5.5) (1.68.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb==0.5.5)\n",
            "  Downloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.5.5) (0.13.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb==0.5.5)\n",
            "  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.5.5) (9.0.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.5.5) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb==0.5.5)\n",
            "  Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.5.5) (3.10.11)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.5.5) (0.27.2)\n",
            "Collecting langchain-core<0.3,>=0.1.40 (from langchain-chroma==0.1.2)\n",
            "  Downloading langchain_core-0.2.43-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.11) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.11) (3.11.2)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.11) (4.0.3)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.11) (0.1.143)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.11) (2.32.3)\n",
            "Collecting tenacity>=8.2.3 (from chromadb==0.5.5)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community==0.2.10)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting groq<1,>=0.4.1 (from langchain-groq==0.1.6)\n",
            "  Downloading groq-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.43.2) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers==4.43.2) (0.26.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.43.2) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.43.2) (2024.9.11)\n",
            "Collecting tokenizers>=0.13.2 (from chromadb==0.5.5)\n",
            "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.43.2) (0.4.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==3.0.1) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==3.0.1) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==3.0.1) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==3.0.1) (11.0.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured==0.15.0) (5.2.0)\n",
            "Collecting filetype (from unstructured==0.15.0)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting python-magic (from unstructured==0.15.0)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured==0.15.0) (5.3.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured==0.15.0) (3.9.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured==0.15.0) (0.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured==0.15.0) (4.12.3)\n",
            "Collecting emoji (from unstructured==0.15.0)\n",
            "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting python-iso639 (from unstructured==0.15.0)\n",
            "  Downloading python_iso639-2024.10.22-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting langdetect (from unstructured==0.15.0)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rapidfuzz (from unstructured==0.15.0)\n",
            "  Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting backoff (from unstructured==0.15.0)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting unstructured-client (from unstructured==0.15.0)\n",
            "  Downloading unstructured_client-0.28.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured==0.15.0) (1.16.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unstructured==0.15.0) (5.9.5)\n",
            "Collecting onnx (from unstructured[pdf]==0.15.0)\n",
            "  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting pdf2image (from unstructured[pdf]==0.15.0)\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting pdfminer.six (from unstructured[pdf]==0.15.0)\n",
            "  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting pikepdf (from unstructured[pdf]==0.15.0)\n",
            "  Downloading pikepdf-9.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.2 kB)\n",
            "Collecting pillow-heif (from unstructured[pdf]==0.15.0)\n",
            "  Downloading pillow_heif-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.8 kB)\n",
            "Collecting pypdf (from unstructured[pdf]==0.15.0)\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting pytesseract (from unstructured[pdf]==0.15.0)\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting google-cloud-vision (from unstructured[pdf]==0.15.0)\n",
            "  Downloading google_cloud_vision-3.8.1-py2.py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting effdet (from unstructured[pdf]==0.15.0)\n",
            "  Downloading effdet-0.4.1-py3-none-any.whl.metadata (33 kB)\n",
            "Collecting unstructured-inference==0.7.36 (from unstructured[pdf]==0.15.0)\n",
            "  Downloading unstructured_inference-0.7.36-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting unstructured.pytesseract>=0.3.12 (from unstructured[pdf]==0.15.0)\n",
            "  Downloading unstructured.pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting layoutparser (from unstructured-inference==0.7.36->unstructured[pdf]==0.15.0)\n",
            "  Downloading layoutparser-0.3.4-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting python-multipart (from unstructured-inference==0.7.36->unstructured[pdf]==0.15.0)\n",
            "  Downloading python_multipart-0.0.17-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[pdf]==0.15.0) (4.10.0.84)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[pdf]==0.15.0) (3.8.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[pdf]==0.15.0) (1.0.11)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.11) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.11) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.11) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.11) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.11) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.11) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.11) (1.17.2)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb==0.5.5)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb==0.5.5) (2.1.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.10)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.10)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting starlette<0.42.0,>=0.40.0 (from fastapi>=0.95.2->chromadb==0.5.5)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq==0.1.6) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq==0.1.6) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq==0.1.6) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb==0.5.5) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb==0.5.5) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb==0.5.5) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb==0.5.5) (0.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.43.2) (2024.10.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb==0.5.5) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb==0.5.5) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb==0.5.5) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb==0.5.5) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb==0.5.5) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb==0.5.5) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb==0.5.5) (2.2.3)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb==0.5.5)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.40->langchain-chroma==0.1.2) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.11) (1.0.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb==0.5.5)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.5.5) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.5.5) (4.25.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.5.5) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb==0.5.5) (1.2.15)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb==0.5.5) (8.5.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.5.5) (1.66.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.28.2 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.5.5)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.28.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.28.2 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.5.5)\n",
            "  Downloading opentelemetry_proto-1.28.2-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting protobuf (from onnxruntime>=1.14.1->chromadb==0.5.5)\n",
            "  Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.5.5)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.49b2-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-instrumentation==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.5.5)\n",
            "  Downloading opentelemetry_instrumentation-0.49b2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.49b2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.5.5) (0.49b2)\n",
            "Collecting opentelemetry-util-http==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.5.5)\n",
            "  Downloading opentelemetry_util_http-0.49b2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.49b2->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.5.5)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb==0.5.5)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb==0.5.5) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb==0.5.5) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.2.11) (3.4.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.2.11) (3.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (3.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.5.5) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb==0.5.5) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb==0.5.5) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb==0.5.5) (13.9.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb==0.5.5)\n",
            "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.5.5)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb==0.5.5)\n",
            "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.5.5)\n",
            "  Downloading watchfiles-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb==0.5.5)\n",
            "  Downloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured==0.15.0) (2.6)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from effdet->unstructured[pdf]==0.15.0) (0.20.1+cu121)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from effdet->unstructured[pdf]==0.15.0) (2.0.8)\n",
            "Collecting omegaconf>=2.0 (from effdet->unstructured[pdf]==0.15.0)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[pdf]==0.15.0) (2.19.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[pdf]==0.15.0) (1.25.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured==0.15.0) (1.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[pdf]==0.15.0) (43.0.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers==3.0.1) (3.5.0)\n",
            "Collecting aiofiles>=24.1.0 (from unstructured-client->unstructured==0.15.0)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured==0.15.0) (0.2.0)\n",
            "Collecting jsonpath-python<2.0.0,>=1.0.6 (from unstructured-client->unstructured==0.15.0)\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured==0.15.0) (1.6.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq==0.1.6) (1.2.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[pdf]==0.15.0) (1.17.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[pdf]==0.15.0) (1.62.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.5.5) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.5.5) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.5.5) (4.9)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb==0.5.5) (3.21.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.40->langchain-chroma==0.1.2) (3.0.0)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0->effdet->unstructured[pdf]==0.15.0)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[pdf]==0.15.0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[pdf]==0.15.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[pdf]==0.15.0) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[pdf]==0.15.0) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[pdf]==0.15.0) (3.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.5.5) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.5.5) (2.18.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.10)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.5.5)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers==3.0.1) (3.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from layoutparser->unstructured-inference==0.7.36->unstructured[pdf]==0.15.0) (2.2.2)\n",
            "Collecting iopath (from layoutparser->unstructured-inference==0.7.36->unstructured[pdf]==0.15.0)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pdfplumber (from layoutparser->unstructured-inference==0.7.36->unstructured[pdf]==0.15.0)\n",
            "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[pdf]==0.15.0) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb==0.5.5) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.5.5) (0.6.1)\n",
            "Collecting portalocker (from iopath->layoutparser->unstructured-inference==0.7.36->unstructured[pdf]==0.15.0)\n",
            "  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->layoutparser->unstructured-inference==0.7.36->unstructured[pdf]==0.15.0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->layoutparser->unstructured-inference==0.7.36->unstructured[pdf]==0.15.0) (2024.2)\n",
            "Collecting pdfminer.six (from unstructured[pdf]==0.15.0)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber->layoutparser->unstructured-inference==0.7.36->unstructured[pdf]==0.15.0)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chromadb-0.5.5-py3-none-any.whl (584 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m584.3/584.3 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_chroma-0.1.2-py3-none-any.whl (9.3 kB)\n",
            "Downloading langchain-0.2.11-py3-none-any.whl (990 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.3/990.3 kB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.2.10-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Downloading langchain_groq-0.1.6-py3-none-any.whl (14 kB)\n",
            "Downloading transformers-4.43.2-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured-0.15.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_inference-0.7.36-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading fastapi-0.115.5-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groq-0.12.0-py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.9/108.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.2.43-py3-none-any.whl (397 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.1/397.1 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.28.2-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.28.2-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.28.2-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.49b2-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.49b2-py3-none-any.whl (30 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.49b2-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_util_http-0.49b2-py3-none-any.whl (6.9 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.7.3-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured.pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading uvicorn-0.32.1-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading effdet-0.4.1-py3-none-any.whl (112 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_cloud_vision-3.8.1-py2.py3-none-any.whl (486 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.9/486.9 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Downloading pikepdf-9.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_heif-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading python_iso639-2024.10.22-py3-none-any.whl (274 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_client-0.28.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Downloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.6/442.6 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading layoutparser-0.3.4-py3-none-any.whl (19.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading python_multipart-0.0.17-py3-none-any.whl (24 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: pypika, langdetect, antlr4-python3-runtime, iopath\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=cbe064e8ad3a680e12ef47eaad9492d1e1a5e23e840bbef294b19436881c1f33\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=d1d2e5cd1346e7eb6b52c4d9c85875802f01aa8e1eea44c7257d7a301df22284\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=894d37ff6c3cc2935d6c52f86c9bd9a79d188a9e296d9d1f3fc6bb3e87e69881\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31528 sha256=bfd53dd9946c62160a9b2965970176977baaba5b0c448e8cbe229f481e3b6d22\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built pypika langdetect antlr4-python3-runtime iopath\n",
            "Installing collected packages: pypika, monotonic, filetype, durationpy, antlr4-python3-runtime, websockets, uvloop, uvicorn, unstructured.pytesseract, tenacity, rapidfuzz, python-multipart, python-magic, python-iso639, python-dotenv, pytesseract, pyproject_hooks, pypdfium2, pypdf, protobuf, portalocker, pillow-heif, pdf2image, overrides, opentelemetry-util-http, omegaconf, mypy-extensions, mmh3, marshmallow, langdetect, jsonpath-python, humanfriendly, httptools, emoji, chroma-hnswlib, bcrypt, backoff, asgiref, aiofiles, watchfiles, typing-inspect, starlette, posthog, pikepdf, opentelemetry-proto, onnx, iopath, coloredlogs, build, unstructured-client, tokenizers, pdfminer.six, opentelemetry-exporter-otlp-proto-common, onnxruntime, kubernetes, groq, fastapi, dataclasses-json, unstructured, transformers, pdfplumber, opentelemetry-instrumentation, langchain-core, sentence-transformers, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, layoutparser, langchain-text-splitters, langchain-groq, google-cloud-vision, effdet, unstructured-inference, opentelemetry-instrumentation-fastapi, langchain, langchain-community, chromadb, langchain-chroma\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.5\n",
            "    Uninstalling protobuf-4.25.5:\n",
            "      Successfully uninstalled protobuf-4.25.5\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.20.3\n",
            "    Uninstalling tokenizers-0.20.3:\n",
            "      Successfully uninstalled tokenizers-0.20.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.46.2\n",
            "    Uninstalling transformers-4.46.2:\n",
            "      Successfully uninstalled transformers-4.46.2\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.19\n",
            "    Uninstalling langchain-core-0.3.19:\n",
            "      Successfully uninstalled langchain-core-0.3.19\n",
            "  Attempting uninstall: sentence-transformers\n",
            "    Found existing installation: sentence-transformers 3.2.1\n",
            "    Uninstalling sentence-transformers-3.2.1:\n",
            "      Successfully uninstalled sentence-transformers-3.2.1\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.2\n",
            "    Uninstalling langchain-text-splitters-0.3.2:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.2\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.7\n",
            "    Uninstalling langchain-0.3.7:\n",
            "      Successfully uninstalled langchain-0.3.7\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.28.3 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.28.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-24.1.0 antlr4-python3-runtime-4.9.3 asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.1 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.5.5 coloredlogs-15.0.1 dataclasses-json-0.6.7 durationpy-0.9 effdet-0.4.1 emoji-2.14.0 fastapi-0.115.5 filetype-1.2.0 google-cloud-vision-3.8.1 groq-0.12.0 httptools-0.6.4 humanfriendly-10.0 iopath-0.1.10 jsonpath-python-1.0.6 kubernetes-31.0.0 langchain-0.2.11 langchain-chroma-0.1.2 langchain-community-0.2.10 langchain-core-0.2.43 langchain-groq-0.1.6 langchain-text-splitters-0.2.2 langdetect-1.0.9 layoutparser-0.3.4 marshmallow-3.23.1 mmh3-5.0.1 monotonic-1.6 mypy-extensions-1.0.0 omegaconf-2.3.0 onnx-1.17.0 onnxruntime-1.20.1 opentelemetry-exporter-otlp-proto-common-1.28.2 opentelemetry-exporter-otlp-proto-grpc-1.28.2 opentelemetry-instrumentation-0.49b2 opentelemetry-instrumentation-asgi-0.49b2 opentelemetry-instrumentation-fastapi-0.49b2 opentelemetry-proto-1.28.2 opentelemetry-util-http-0.49b2 overrides-7.7.0 pdf2image-1.17.0 pdfminer.six-20231228 pdfplumber-0.11.4 pikepdf-9.4.2 pillow-heif-0.20.0 portalocker-3.0.0 posthog-3.7.3 protobuf-5.28.3 pypdf-5.1.0 pypdfium2-4.30.0 pypika-0.48.9 pyproject_hooks-1.2.0 pytesseract-0.3.13 python-dotenv-1.0.1 python-iso639-2024.10.22 python-magic-0.4.27 python-multipart-0.0.17 rapidfuzz-3.10.1 sentence-transformers-3.0.1 starlette-0.41.3 tenacity-8.5.0 tokenizers-0.19.1 transformers-4.43.2 typing-inspect-0.9.0 unstructured-0.15.0 unstructured-client-0.28.0 unstructured-inference-0.7.36 unstructured.pytesseract-0.3.13 uvicorn-0.32.1 uvloop-0.21.0 watchfiles-1.0.0 websockets-14.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "pydevd_plugins"
                ]
              },
              "id": "b452e53347ad4e40bbedc0e1579202df"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# After running this code the KERNEL needs to be restarted\n",
        "\n",
        "!pip install chromadb==0.5.5 langchain-chroma==0.1.2 langchain==0.2.11 langchain-community==0.2.10 langchain-text-splitters==0.2.2 langchain-groq==0.1.6 transformers==4.43.2 sentence-transformers==3.0.1 unstructured==0.15.0 unstructured[pdf]==0.15.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiVXKicZYnkb"
      },
      "source": [
        "Run the code snippet below it has every library included."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpIE0uyhHGMw",
        "outputId": "c990316a-e144-445d-8731-987a6535f80d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/models/WordEmbeddings.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  weights = torch.load(os.path.join(input_path, \"pytorch_model.bin\"), map_location=torch.device(\"cpu\"))\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "# Langchain modules\n",
        "from langchain.document_loaders import UnstructuredFileLoader, PyPDFDirectoryLoader, TextLoader\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.text_splitter import (\n",
        "    CharacterTextSplitter,\n",
        "    RecursiveCharacterTextSplitter,\n",
        "    SentenceTransformersTokenTextSplitter,\n",
        "    TokenTextSplitter,\n",
        ")\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "\n",
        "# Set API Key for Groq\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "\n",
        "\n",
        "# Download Sentence Transformers Embedding From Hugging Face\n",
        "#embeddings = HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2')\n",
        "embeddings = HuggingFaceEmbeddings(model_name = 'sentence-transformers/average_word_embeddings_komninos') #best\n",
        "#embeddings = HuggingFaceEmbeddings() # Make sure you use the same embedders as you used to embed the original dataset into vector database.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFSTz6C2VKZC"
      },
      "source": [
        "There are two ways to read and predict in the model, Manual and Automatic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67jibqw5qi90"
      },
      "source": [
        "# Manual entry of vector database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDRhA5ANHu2B",
        "outputId": "5a1ae1f1-161d-473e-8ea8-2d4fbe36dfcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/auto_2001_new_embed_full.zip...\n",
            "Extraction completed for auto_2001_new_embed_full.zip!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Directory containing the zip files\n",
        "zip_dir = \"/content/\"\n",
        "extract_path = \"/content/\"  # Extraction path\n",
        "\n",
        "# Iterate through all files in the directory\n",
        "for file_name in os.listdir(zip_dir):\n",
        "    if file_name.endswith(\".zip\"):  # Check if the file is a .zip file\n",
        "        zip_path = os.path.join(zip_dir, file_name)\n",
        "        print(f\"Extracting {zip_path}...\")\n",
        "\n",
        "        # Unzip the file\n",
        "        with ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_path)\n",
        "            print(f\"Extraction completed for {file_name}!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bV52In5xdunk"
      },
      "outputs": [],
      "source": [
        "# Retrieving the first vector database\n",
        "persist_directory_1 = \"/content/content/auto_book_1\"\n",
        "vectordb_1 = Chroma(\n",
        "    persist_directory=persist_directory_1,\n",
        "    embedding_function=embeddings\n",
        ")\n",
        "\n",
        "# Retrieving the second vector database\n",
        "persist_directory_2 = \"/content/content/dheer_auto_parts_db\"\n",
        "vectordb_2 = Chroma(\n",
        "    persist_directory=persist_directory_2,\n",
        "    embedding_function=embeddings\n",
        ")\n",
        "\"\"\"\n",
        "# Retrieving the third vector database\n",
        "persist_directory_3 = \"/content/content/auto_book_2\"\n",
        "vectordb_3 = Chroma(\n",
        "    persist_directory=persist_directory_3,\n",
        "    embedding_function=embeddings\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "# Create retrievers for both vector databases\n",
        "retriever_1 = vectordb_1.as_retriever()\n",
        "retriever_2 = vectordb_2.as_retriever()\n",
        "#retriever_3 = vectordb_3.as_retriever()\n",
        "\n",
        "# Define a combined retriever function\n",
        "def combined_retriever(query):\n",
        "    # Retrieve documents from both retrievers\n",
        "    results_1 = retriever_1.get_relevant_documents(query)\n",
        "    results_2 = retriever_2.get_relevant_documents(query)\n",
        "    #results_3 = retriever_3.get_relevant_documents(query)\n",
        "\n",
        "    # Combine results (you can sort or filter if needed)\n",
        "    combined_results = results_1 + results_2 #+ results_3\n",
        "    return combined_results\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.2-90b-vision-preview\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# Load a question-answering chain using the \"stuff\" chain type\n",
        "qa_chain = load_qa_chain(llm, chain_type=\"map_reduce\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUnCdES5HPsF",
        "outputId": "26d8d489-4905-4316-cfae-2e0f6d4b7b39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to the Car Issue Chatbot! Type 'exit' to end the conversation.\n",
            "\n",
            " **You**: I have 2011 toyota rav 4 and my brakes are not working\n",
            "\n",
            " **SBG**: I understand you're having issues with your 2011 Toyota RAV4's brakes. I've found some relevant information regarding brake performance. According to the document, \"Some brakes stop well when they are cold.\" This might be related to your issue.\n",
            "\n",
            "For a 2011 Toyota RAV4, the brake pads cost around $50-$70 per set. However, I would recommend checking the brake rotors as well, which cost around $200-$300 per set.\n",
            "\n",
            "Some related subcategories for brake parts include:\n",
            "\n",
            "- Brake pads ($50-$70 per set)\n",
            "- Brake rotors ($200-$300 per set)\n",
            "- Brake fluid (available, $10-$20 per liter)\n",
            "\n",
            "Please note that these prices are estimates and may vary depending on the specific part and brand.\n",
            "\n",
            "Additionally, I would like to ask if you have checked the brake fluid level and condition in your vehicle?\n",
            "\n",
            " **You**: waht about hydrolics ?\n",
            "\n",
            " **SBG**: I understand you're inquiring about the hydraulic system in your 2011 Toyota RAV4, specifically in relation to the brake issue you mentioned earlier.\n",
            "\n",
            "For a 2011 Toyota RAV4, the hydraulic brake master cylinder costs around $150-$250. However, I would recommend checking the brake fluid level and condition, as I mentioned earlier, as well as the brake lines and hoses, which cost around $50-$100 each.\n",
            "\n",
            "Some related subcategories for hydraulic brake parts include:\n",
            "\n",
            "- Brake master cylinder ($150-$250)\n",
            "- Brake lines and hoses ($50-$100 each)\n",
            "- Brake fluid (available, $10-$20 per liter)\n",
            "\n",
            "Please note that these prices are estimates and may vary depending on the specific part and brand.\n",
            "\n",
            "Additionally, I would like to ask if you have noticed any signs of hydraulic fluid leaks or issues with the brake pedal feel?\n",
            "\n",
            " **You**: Yes.\n",
            "\n",
            " **SBG**: I understand you've noticed signs of hydraulic fluid leaks or issues with the brake pedal feel in your 2011 Toyota RAV4. This could be related to the brake master cylinder or brake lines and hoses.\n",
            "\n",
            "Considering the hydraulic system, I would recommend checking the brake calipers as well. For a 2011 Toyota RAV4, the brake calipers might cost around $100-$200 each, depending on the brand and quality.\n",
            "\n",
            "Some related subcategories for brake parts include:\n",
            "\n",
            "- Brake calipers ($100-$200 each)\n",
            "- Brake pads ($50-$70 per set)\n",
            "- Brake rotors ($200-$300 per set)\n",
            "- Brake fluid (available, $10-$20 per liter)\n",
            "\n",
            "Please note that these prices are estimates and may vary depending on the specific part and brand.\n",
            "\n",
            "Additionally, I would recommend checking the brake fluid level and condition, as well as the brake lines and hoses, to ensure there are no leaks or issues that could be causing the brake pedal feel problem.\n",
            "\n",
            "If you'd like to proceed with replacing the brake calipers, I can try to find more specific pricing information for you.\n",
            "\n",
            " **You**: Can you tell me which Engine oil my car uses?\n",
            "\n",
            " **SBG**: I understand you're inquiring about the engine oil for your 2011 Toyota RAV4. \n",
            "\n",
            "Typically, Toyota RAV4 models use 0W-20 or 5W-20 synthetic oil. However, I would recommend checking your owner's manual or the oil filler cap on your engine to confirm the recommended oil type.\n",
            "\n",
            "For a 2011 Toyota RAV4, the engine oil costs around $20-$50 per liter, depending on the brand and quality.\n",
            "\n",
            "Some related subcategories for engine oil include:\n",
            "\n",
            "- 0W-20 synthetic oil (available, $20-$50 per liter)\n",
            "- 5W-20 synthetic oil (available, $20-$50 per liter)\n",
            "- Oil filters (available, $5-$15 each)\n",
            "\n",
            "Please note that these prices are estimates and may vary depending on the specific part and brand.\n",
            "\n",
            "Additionally, I would like to ask if you have checked the engine oil level and condition in your vehicle recently?\n",
            "\n",
            " **You**: exit\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ],
      "source": [
        "# Create a prompt template\n",
        "chatbot_prompt = \"\"\"\n",
        "You are an automotive parts assistant. When a user asks about their vehicle, you will refer to relevant documents and provide guidance in a concise, clear manner. Your goal is to stay under 1,000 tokens for each response, including all necessary details.\n",
        "\n",
        "Here are the steps to follow:\n",
        "\n",
        "1. **Identify the Vehicle and Parts**: Determine what part the user is asking about based on their question. If it’s unclear, ask clarifying questions to understand the model, make, and year of the vehicle.\n",
        "\n",
        "2. **Provide Pricing**: Always provide the price for the part requested. If you don’t have the exact year of the vehicle the user asks about, provide the price for the earliest year available in your database and inform the user that you cannot help with the exact year requested. You should always mention the earliest year available, even if the part is not available for the requested year.\n",
        "\n",
        "3. **Include Related Subcategories**: If the part requested falls under a category that has subcategories (e.g., \"engine parts\" has \"fuel injectors\", \"oil filters\", etc.), list those subcategories with their prices, if available.\n",
        "\n",
        "4. **Mention Fluids**: If the part requested is related to fluids (e.g., oil, transmission fluid), also mention the fluids associated with the part and their availability/price.\n",
        "\n",
        "5. **Maintain Chat Context**: Keep track of previous conversations and refer to them when necessary to provide consistent follow-up answers. For example, if the user has already asked about a part and later asks about another part from the same vehicle, refer back to previous details such as model, year, or part-related info.\n",
        "\n",
        "6. **Structure the Response**:\n",
        "   - Begin by acknowledging the vehicle type and confirming details, especially if the model year or make was mentioned.\n",
        "   - Provide the price for the part.\n",
        "   - List any relevant subcategories and related fluids.\n",
        "   - If no exact match for the year is found, state the earliest year available in the database and explain the limitation.\n",
        "\n",
        "**Keep responses concise, with no more than 1,000 tokens. If the response exceeds the token limit, trim unnecessary details.**\n",
        "\n",
        "Conversation so far:\n",
        "{chat_history}\n",
        "\n",
        "User's question:\n",
        "{user_input}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Define the chatbot function\n",
        "def chatbot():\n",
        "    print(\"Welcome to the Car Issue Chatbot! Type 'exit' to end the conversation.\")\n",
        "\n",
        "    # Initialize chat history\n",
        "    chat_history = []\n",
        "\n",
        "    while True:\n",
        "        # Get user input\n",
        "        query = input(\"\\n **You**: \")\n",
        "\n",
        "        # Exit condition\n",
        "        if query.lower() == \"exit\":\n",
        "            print(\"Chatbot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Automatically add the user message to the chat history\n",
        "        chat_history.append({\"role\": \"user\", \"content\": query})\n",
        "\n",
        "        # Create prompt for the current conversation context\n",
        "        prompt = chatbot_prompt.format(\n",
        "            chat_history=\"\\n\".join([f\"{message['role'].capitalize()}: {message['content']}\" for message in chat_history]),\n",
        "            user_input=query\n",
        "        )\n",
        "\n",
        "        # Retrieve documents from both vector databases\n",
        "        combined_results = combined_retriever(query)\n",
        "\n",
        "        # Pass the combined results to the chain\n",
        "        response = qa_chain.run(\n",
        "            input_documents=combined_results,\n",
        "            question=prompt  # Passing the user query directly\n",
        "        )\n",
        "\n",
        "        # Print the result\n",
        "        print(f\"\\n **SBG**: {response}\")\n",
        "\n",
        "        # Add assistant's response to chat history automatically\n",
        "        chat_history.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "# Run the chatbot\n",
        "chatbot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_7rmTjvhQKx"
      },
      "source": [
        "# Automatic entry of vector database\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCfKZTnAXT2Y"
      },
      "source": [
        "## Unziping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBjsWy7hXSRQ",
        "outputId": "74e0ec94-1f8a-494c-d662-19bedf1ce6a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/auto_book1_new_embed_full.zip...\n",
            "Extraction completed for auto_book1_new_embed_full.zip!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Directory containing the zip files\n",
        "zip_dir = \"/content/\"\n",
        "extract_path = \"/content/\"  # Extraction path\n",
        "\n",
        "# Iterate through all files in the directory\n",
        "for file_name in os.listdir(zip_dir):\n",
        "    if file_name.endswith(\".zip\"):  # Check if the file is a .zip file\n",
        "        zip_path = os.path.join(zip_dir, file_name)\n",
        "        print(f\"Extracting {zip_path}...\")\n",
        "\n",
        "        # Unzip the file\n",
        "        with ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_path)\n",
        "            print(f\"Extraction completed for {file_name}!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycj2W0t-XZAQ"
      },
      "source": [
        "## Reading the vector database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJft15bqewgL",
        "outputId": "9feef698-a9ab-490e-8981-8901dc55293d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading vector database from: /content/content/auto_2000_new_embed_full\n",
            "Loading vector database from: /content/content/.ipynb_checkpoints\n",
            "Source document:  /content/all_data_2000.csv\n",
            "Retrieved 4 documents.\n"
          ]
        }
      ],
      "source": [
        "#!pip install chromadb\n",
        "import os\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# Directory containing vector databases\n",
        "vector_db_dir = \"/content/content/\"\n",
        "embedding_function = embeddings  # Ensure this is defined earlier in your code\n",
        "\n",
        "# Initialize a list to store retrievers\n",
        "retrievers = []\n",
        "\n",
        "# Iterate through all subdirectories in the specified directory\n",
        "for sub_dir in os.listdir(vector_db_dir):\n",
        "    full_path = os.path.join(vector_db_dir, sub_dir)\n",
        "    if os.path.isdir(full_path):  # Check if it's a directory\n",
        "        print(f\"Loading vector database from: {full_path}\")\n",
        "\n",
        "        # Load the vector database\n",
        "        vectordb = Chroma(\n",
        "            persist_directory=full_path,\n",
        "            embedding_function=embedding_function\n",
        "        )\n",
        "\n",
        "        # Create a retriever and add it to the list\n",
        "        retrievers.append(vectordb.as_retriever())\n",
        "\n",
        "# Define a combined retriever function\n",
        "def combined_retriever(query):\n",
        "    combined_results = []\n",
        "    for retriever in retrievers:\n",
        "        # Retrieve documents from each retriever\n",
        "        combined_results.extend(retriever.get_relevant_documents(query))\n",
        "\n",
        "    return combined_results\n",
        "\n",
        "# Example Usage\n",
        "query = \"What is the price of a fuel injector for 2001 toyota rav4 ?\"\n",
        "results = combined_retriever(query)\n",
        "if results:\n",
        "    print(\"Source document: \", results[0].metadata[\"source\"])\n",
        "\n",
        "# Print the number of documents retrieved\n",
        "print(f\"Retrieved {len(results)} documents.\")\n",
        "\n",
        "# Optionally print the source document (first result)\n",
        "\n",
        "\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.2-90b-vision-preview\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# Load a question-answering chain using the \"stuff\" chain type\n",
        "qa_chain = load_qa_chain(llm, chain_type=\"refine\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RbbT-qgXg46"
      },
      "source": [
        "## Chatbot finetuning and chatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FrqznTvehiek",
        "outputId": "fbd0ec3c-745e-4078-a4dd-ca6f66e9dae2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the Car Issue Chatbot! Type 'exit' to end the conversation.\n",
            "\n",
            " **You**: 2000 Tyota rav4 brakes\n",
            "\n",
            " **SBG**: For your 2000 Toyota RAV4 with a 2.4L engine and automatic transmission, I found the drum brakes part.\n",
            "\n",
            "Part Category: Braking System\n",
            "Part Name: Drum Brakes\n",
            "Part Manufacturer: Brembo\n",
            "Part Number: BODRU1960\n",
            "Price: $216.66\n",
            "\n",
            "Would you like to know more about this part or check for other brake-related components, such as brake pads or rotors?\n",
            "\n",
            " **You**: manufacturer for the parts\n",
            "\n",
            " **SBG**: The new context is not relevant to the original question. The original question was about the manufacturer for the 2000 Toyota RAV4 drum brakes.\n",
            "\n",
            "The original answer remains the same:\n",
            "\n",
            " For the 2000 Toyota RAV4 drum brakes, the part manufacturer is Brembo.\n",
            "\n",
            " **You**: exit\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ],
      "source": [
        "# Create a prompt template\n",
        "chatbot_prompt = \"\"\"\n",
        "You are an intelligent and efficient **Automotive Parts and Fluids Assistant**. Your goal is to assist customers by identifying the most relevant parts and fluids for their vehicles based on their questions or descriptions. Always prioritize clarity, accuracy, and customer satisfaction. Follow these guidelines:\n",
        "If a customer says something generic like, “My car isn’t starting,” the issue could be from multiple causes such as the alternator, battery, starter, oil leaks, etc. the chatbot would ask clarifying questions which are very helpful for initial inspection such as “Does the engine turn over?” or “Did you notice oil drips on your driveway?” or “Did the car stop after reaching 90 km/h?”. The chatbot would then provide an estimate for the mandatory \"No Start\" inspection and, based on the customer’s responses, suggest the top three most likely repair orders along with the total cost. The model would continuously optimize itself by learning from feedback given by auto service advisors on the repair orders.\n",
        "\n",
        "On the auto parts side, the bot could easily provide customers with the parts they ask for specifically when customers are unsure of the exact name of a part, they can describe the parts and based on it the chatbot would give top 3 parts that matches with description given by customer. It would also show availability, different manufacturers and prices, stock levels, and warranty details in real-time.\n",
        "**Keep responses concise, with no more than 1,000 tokens. If the response exceeds the token limit, trim unnecessary details.**\n",
        "\n",
        "Conversation so far:\n",
        "{chat_history}\n",
        "\n",
        "User's question:\n",
        "{user_input}\n",
        "\"\"\"\n",
        "\n",
        "# Define the chatbot function\n",
        "def chatbot():\n",
        "    print(\"Welcome to the Car Issue Chatbot! Type 'exit' to end the conversation.\")\n",
        "\n",
        "    # Initialize chat history\n",
        "    chat_history = []\n",
        "\n",
        "    while True:\n",
        "        # Get user input\n",
        "        query = input(\"\\n **You**: \")\n",
        "\n",
        "        # Exit condition\n",
        "        if query.lower() == \"exit\":\n",
        "            print(\"Chatbot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Automatically add the user message to the chat history\n",
        "        chat_history.append({\"role\": \"user\", \"content\": query})\n",
        "\n",
        "        # Create prompt for the current conversation context\n",
        "        prompt = chatbot_prompt.format(\n",
        "            chat_history=\"\\n\".join([f\"{message['role'].capitalize()}: {message['content']}\" for message in chat_history]),\n",
        "            user_input=query\n",
        "        )\n",
        "\n",
        "        # Retrieve documents from both vector databases\n",
        "        combined_results = combined_retriever(query)\n",
        "\n",
        "        # Pass the combined results to the chain\n",
        "        response = qa_chain.run(\n",
        "            input_documents=combined_results,\n",
        "            question=prompt  # Passing the user query directly\n",
        "        )\n",
        "\n",
        "        # Print the result\n",
        "        print(f\"\\n **SBG**: {response}\")\n",
        "\n",
        "        # Add assistant's response to chat history automatically\n",
        "        chat_history.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "# Run the chatbot\n",
        "chatbot()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One for all\n",
        "Just one retriever.\n"
      ],
      "metadata": {
        "id": "j_6ydqDmUqKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below stores previous chat and respond accordingly. Have to work on prompting\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WI2qOKZrYq-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Sentence Transformers Embedding From Hugging Face\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# Initialize embeddings, vector store, retriever, and LLM\n",
        "embeddings = HuggingFaceEmbeddings(model_name = 'sentence-transformers/average_word_embeddings_komninos')\n",
        "persist_directory = \"/content/content/auto_2000_new_embed_full\"\n",
        "\n",
        "# Initialize Chroma vector store\n",
        "vectordb = Chroma(\n",
        "    persist_directory=persist_directory,\n",
        "    embedding_function=embeddings\n",
        ")\n",
        "\n",
        "# Set up the retriever\n",
        "retriever = vectordb.as_retriever()\n",
        "\n",
        "# LLM from Groq\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.1-70b-versatile\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# Configuration for conversation buffer memory\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True, output_key=\"result\")\n",
        "\n",
        "# Create the QA chain with memory\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        "    memory=memory  # Pass the memory here\n",
        ")\n",
        "\n",
        "# Interactive loop for querying\n",
        "print(\"Welcome to the chatbot. Type 'exit' to stop.\")\n",
        "while True:\n",
        "    query = input(\"You: \")\n",
        "\n",
        "    if query.lower() == 'exit':\n",
        "        print(\"Exiting the chatbot. Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Get the response from the QA chain\n",
        "    response = qa_chain.invoke({\"query\": query})\n",
        "\n",
        "    # Print the result\n",
        "    print(\"Bot: \", response[\"result\"])\n",
        "\n",
        "    # Optionally print the source document (first result)\n",
        "    if response.get(\"source_documents\"):\n",
        "        print(\"Source document: \", response[\"source_documents\"][0].metadata[\"source\"])\n",
        "\n",
        "    # Debug: Show current conversation memory (optional)\n",
        "    print(\"\\nMemory so far:\\n\", memory.buffer)"
      ],
      "metadata": {
        "id": "vLQH_I3JUz5Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "05c0d94d-8096-42da-dfac-ba0929c5c8d5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/models/WordEmbeddings.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  weights = torch.load(os.path.join(input_path, \"pytorch_model.bin\"), map_location=torch.device(\"cpu\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the chatbot. Type 'exit' to stop.\n",
            "You: 2000 Audi A4 brakes\n",
            "Bot:  I don't have information about the 2000 Audi A4 brakes. The provided context is about the 2000 Audi A3, not the A4.\n",
            "Source document:  /content/all_data_2000.csv\n",
            "\n",
            "Memory so far:\n",
            " [HumanMessage(content='2000 Audi A4 brakes'), AIMessage(content=\"I don't have information about the 2000 Audi A4 brakes. The provided context is about the 2000 Audi A3, not the A4.\")]\n",
            "You: 2000 Audi A3 brakes\n",
            "Bot:  For a 2000 Audi A3, I found information on the following brake options:\n",
            "\n",
            "1. Drum Brakes:\n",
            "   - Engine Size: 1.8L\n",
            "   - Part Manufacturer: Brembo\n",
            "   - Part Number: BODRU8676\n",
            "   - Price: $200.5\n",
            "\n",
            "2. Disc Brakes:\n",
            "   - Engine Size: 1.8L\n",
            "   - Part Manufacturer: Brembo\n",
            "   - Part Number: BODIS9113\n",
            "   - Price: $212.68\n",
            "\n",
            "3. Drum Brakes (different engine size):\n",
            "   - Engine Size: 5.7L\n",
            "   - Part Manufacturer: Brembo\n",
            "   - Part Number: BODRU8570\n",
            "   - Price: $208.68\n",
            "\n",
            "Please note that the information provided does not include details on the fluid type, subtype, manufacturer, or number.\n",
            "Source document:  /content/all_data_2000.csv\n",
            "\n",
            "Memory so far:\n",
            " [HumanMessage(content='2000 Audi A4 brakes'), AIMessage(content=\"I don't have information about the 2000 Audi A4 brakes. The provided context is about the 2000 Audi A3, not the A4.\"), HumanMessage(content='2000 Audi A3 brakes'), AIMessage(content='For a 2000 Audi A3, I found information on the following brake options:\\n\\n1. Drum Brakes:\\n   - Engine Size: 1.8L\\n   - Part Manufacturer: Brembo\\n   - Part Number: BODRU8676\\n   - Price: $200.5\\n\\n2. Disc Brakes:\\n   - Engine Size: 1.8L\\n   - Part Manufacturer: Brembo\\n   - Part Number: BODIS9113\\n   - Price: $212.68\\n\\n3. Drum Brakes (different engine size):\\n   - Engine Size: 5.7L\\n   - Part Manufacturer: Brembo\\n   - Part Number: BODRU8570\\n   - Price: $208.68\\n\\nPlease note that the information provided does not include details on the fluid type, subtype, manufacturer, or number.')]\n",
            "You: Show me prices from all parts manufacturer\n",
            "Bot:  Based on the provided information, there is only one parts manufacturer mentioned, which is Centric. The prices for the parts from Centric are as follows:\n",
            "\n",
            "- Doors (CCDOO3271): $752.39\n",
            "- Seats (CCSEA9740): $682.46\n",
            "Source document:  /content/all_data_2000.csv\n",
            "\n",
            "Memory so far:\n",
            " [HumanMessage(content='2000 Audi A4 brakes'), AIMessage(content=\"I don't have information about the 2000 Audi A4 brakes. The provided context is about the 2000 Audi A3, not the A4.\"), HumanMessage(content='2000 Audi A3 brakes'), AIMessage(content='For a 2000 Audi A3, I found information on the following brake options:\\n\\n1. Drum Brakes:\\n   - Engine Size: 1.8L\\n   - Part Manufacturer: Brembo\\n   - Part Number: BODRU8676\\n   - Price: $200.5\\n\\n2. Disc Brakes:\\n   - Engine Size: 1.8L\\n   - Part Manufacturer: Brembo\\n   - Part Number: BODIS9113\\n   - Price: $212.68\\n\\n3. Drum Brakes (different engine size):\\n   - Engine Size: 5.7L\\n   - Part Manufacturer: Brembo\\n   - Part Number: BODRU8570\\n   - Price: $208.68\\n\\nPlease note that the information provided does not include details on the fluid type, subtype, manufacturer, or number.'), HumanMessage(content='Show me prices from all parts manufacturer'), AIMessage(content='Based on the provided information, there is only one parts manufacturer mentioned, which is Centric. The prices for the parts from Centric are as follows:\\n\\n- Doors (CCDOO3271): $752.39\\n- Seats (CCSEA9740): $682.46')]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-316bb9d4aeb8>\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Welcome to the chatbot. Type 'exit' to stop.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompting"
      ],
      "metadata": {
        "id": "1nIGgOaJji92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Sentence Transformers Embedding From Hugging Face\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# Initialize embeddings, vector store, retriever, and LLM\n",
        "embeddings = HuggingFaceEmbeddings(model_name = 'sentence-transformers/average_word_embeddings_komninos')\n",
        "persist_directory = \"/content/content/auto_2000_new_embed_full\"\n",
        "\n",
        "# Initialize Chroma vector store\n",
        "vectordb = Chroma(\n",
        "    persist_directory=persist_directory,\n",
        "    embedding_function=embeddings\n",
        ")\n",
        "\n",
        "# Set up the retriever\n",
        "retriever = vectordb.as_retriever()\n",
        "\n",
        "# LLM from Groq\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.1-70b-versatile\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# Configuration for conversation buffer memory\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True, output_key=\"result\")\n",
        "\n",
        "# Define the custom prompt template\n",
        "chatbot_prompt = \"\"\"\n",
        "You are an intelligent and efficient **Automotive Parts and Fluids Assistant**. Your goal is to assist customers by identifying the most relevant parts and fluids for their vehicles based on their questions or descriptions. Always prioritize clarity, accuracy, and customer satisfaction. Follow these guidelines:\n",
        "- If a customer says something generic like, “My car isn’t starting,” the issue could be from multiple causes such as the alternator, battery, starter, oil leaks, etc. The chatbot would ask clarifying questions like “Does the engine turn over?” or “Did you notice oil drips on your driveway?”\n",
        "- For parts, if a customer is unsure of the exact name of a part, they can describe the part and the chatbot should suggest top 3 parts that match the description. It will also provide availability, different manufacturers, prices, stock levels, and warranty details.\n",
        "\n",
        "**Keep responses concise, with no more than 1,000 tokens. If the response exceeds the token limit, trim unnecessary details.**\n",
        "\n",
        "Conversation so far:\n",
        "{chat_history}\n",
        "\n",
        "User's question:\n",
        "{user_input}\n",
        "\"\"\"\n",
        "\n",
        "# Create the QA chain with memory, prompt, and retriever\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        "    memory=memory  # Pass the memory here\n",
        ")\n",
        "\n",
        "# Interactive loop for querying\n",
        "print(\"Welcome to the chatbot. Type 'exit' to stop.\")\n",
        "while True:\n",
        "    query = input(\"You: \")\n",
        "\n",
        "    if query.lower() == 'exit':\n",
        "        print(\"Exiting the chatbot. Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Create the prompt for the current conversation context\n",
        "    prompt = chatbot_prompt.format(\n",
        "        chat_history=\"\\n\".join([f\"{message['role'].capitalize()}: {message['content']}\" for message in memory.buffer]),\n",
        "        user_input=query\n",
        "    )\n",
        "\n",
        "    # Get the response from the QA chain, passing the custom prompt as context\n",
        "    response = qa_chain.invoke({\"query\": prompt})\n",
        "\n",
        "    # Print the result\n",
        "    print(\"Bot: \", response[\"result\"])\n",
        "\n",
        "    # Optionally print the source document (first result)\n",
        "    if response.get(\"source_documents\"):\n",
        "        print(\"Source document: \", response[\"source_documents\"][0].metadata[\"source\"])\n",
        "\n",
        "    # Debug: Show current conversation memory (optional)\n",
        "    print(\"\\nMemory so far:\\n\", memory.buffer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "etGaw8f-i_UG",
        "outputId": "5c4066ab-a9d6-498d-8838-59ee08167b0e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to the chatbot. Type 'exit' to stop.\n",
            "You: 2000 Audi A4 brakes\n",
            "Bot:  I don't have information on a 2000 Audi A4. The data I have is for a 2000 Ford Focus. If you'd like to know more about the Ford Focus, I can try to help. Alternatively, you can provide more details about your Audi A4 brakes, such as the type of brake issue you're experiencing or the specific brake component you're looking for.\n",
            "Source document:  /content/all_data_2000.csv\n",
            "\n",
            "Memory so far:\n",
            " [HumanMessage(content=\"\\nYou are an intelligent and efficient **Automotive Parts and Fluids Assistant**. Your goal is to assist customers by identifying the most relevant parts and fluids for their vehicles based on their questions or descriptions. Always prioritize clarity, accuracy, and customer satisfaction. Follow these guidelines:\\n- If a customer says something generic like, “My car isn’t starting,” the issue could be from multiple causes such as the alternator, battery, starter, oil leaks, etc. The chatbot would ask clarifying questions like “Does the engine turn over?” or “Did you notice oil drips on your driveway?”\\n- For parts, if a customer is unsure of the exact name of a part, they can describe the part and the chatbot should suggest top 3 parts that match the description. It will also provide availability, different manufacturers, prices, stock levels, and warranty details.\\n\\n**Keep responses concise, with no more than 1,000 tokens. If the response exceeds the token limit, trim unnecessary details.**\\n\\nConversation so far:\\n\\n\\nUser's question:\\n2000 Audi A4 brakes\\n\"), AIMessage(content=\"I don't have information on a 2000 Audi A4. The data I have is for a 2000 Ford Focus. If you'd like to know more about the Ford Focus, I can try to help. Alternatively, you can provide more details about your Audi A4 brakes, such as the type of brake issue you're experiencing or the specific brake component you're looking for.\")]\n",
            "You: Brakes for Audi A4 2000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'HumanMessage' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-494703778ca4>\u001b[0m in \u001b[0;36m<cell line: 56>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# Create the prompt for the current conversation context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     prompt = chatbot_prompt.format(\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mchat_history\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"{message['role'].capitalize()}: {message['content']}\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0muser_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     )\n",
            "\u001b[0;32m<ipython-input-3-494703778ca4>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# Create the prompt for the current conversation context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     prompt = chatbot_prompt.format(\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mchat_history\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"{message['role'].capitalize()}: {message['content']}\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0muser_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     )\n",
            "\u001b[0;31mTypeError\u001b[0m: 'HumanMessage' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just for trying using one query"
      ],
      "metadata": {
        "id": "nB7ge4_PYzRs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "collapsed": true,
        "id": "owyYaCYVsuZy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18bfbe87-99ff-4648-f198-d0416ec31bf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/models/WordEmbeddings.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  weights = torch.load(os.path.join(input_path, \"pytorch_model.bin\"), map_location=torch.device(\"cpu\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For a 2000 Toyota RAV4, the prices for brakes are as follows:\n",
            "\n",
            "- 1.8L engine, Automatic transmission:\n",
            "  - Disc Brakes: $189.94\n",
            "  - Drum Brakes: $211.32\n",
            "\n",
            "- 2.4L engine, Automatic transmission:\n",
            "  - Disc Brakes: $194.63\n",
            "  - Drum Brakes: $216.66\n",
            "/content/all_data_2000.csv\n"
          ]
        }
      ],
      "source": [
        "# Download Sentence Transformers Embedding From Hugging Face\n",
        "embeddings = HuggingFaceEmbeddings(model_name = 'sentence-transformers/average_word_embeddings_komninos')\n",
        "persist_directory = \"/content/content/auto_2000_new_embed_full\"\n",
        "from langchain_chroma import Chroma\n",
        "vectordb = Chroma(\n",
        "    persist_directory=persist_directory,\n",
        "    embedding_function=embeddings\n",
        ")\n",
        "\n",
        "\n",
        "# retriever\n",
        "retriever = vectordb.as_retriever()\n",
        "# llm from groq\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.1-70b-versatile\",\n",
        "    temperature=0\n",
        ")\n",
        "config = {\"configurable\": {\"thread_id\": \"dheer123\"}}\n",
        "# create a qa chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        "    #config=config,\n",
        "\n",
        ")\n",
        "# invoke the qa chain and get a response for user query\n",
        "query = \"Prices for brakes of Toyota Rav4 2000\"\n",
        "response = qa_chain.invoke({\"query\": query})\n",
        "print(response[\"result\"])\n",
        "print(response[\"source_documents\"][0].metadata[\"source\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# Directory containing the subdirectories for vector databases\n",
        "vector_db_dir = \"/content/content\"\n",
        "embedding_function = HuggingFaceEmbeddings(model_name='sentence-transformers/average_word_embeddings_komninos')\n",
        "\n",
        "# Initialize a list to store retrievers\n",
        "retrievers = []\n",
        "\n",
        "# Iterate through all subdirectories in the specified directory\n",
        "for sub_dir in os.listdir(vector_db_dir):\n",
        "    full_path = os.path.join(vector_db_dir, sub_dir)\n",
        "    if os.path.isdir(full_path):  # Check if it's a directory\n",
        "        print(f\"Loading vector database from: {full_path}\")\n",
        "\n",
        "        # Load the vector database\n",
        "        vectordb = Chroma(\n",
        "            persist_directory=full_path,\n",
        "            embedding_function=embedding_function\n",
        "        )\n",
        "\n",
        "        # Create a retriever and add it to the list\n",
        "        retrievers.append(vectordb.as_retriever())\n",
        "\n",
        "# Define a combined retriever function\n",
        "def combined_retriever(query):\n",
        "    combined_results = []\n",
        "    for retriever in retrievers:\n",
        "        # Retrieve documents from each retriever\n",
        "        combined_results.extend(retriever.get_relevant_documents(query))\n",
        "\n",
        "    return combined_results\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.1-70b-versatile\",  # Use the appropriate model as per your setup\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# Configuration for conversation buffer memory\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True, output_key=\"result\")\n",
        "\n",
        "# Create the QA chain with memory and combined retriever\n",
        "qa_chain = RetrievalQA.invoke(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=combined_results,\n",
        "    return_source_documents=True,\n",
        "    memory=memory  # Pass the memory here\n",
        ")\n",
        "\n",
        "# Interactive loop for querying\n",
        "print(\"Welcome to the chatbot. Type 'exit' to stop.\")\n",
        "while True:\n",
        "    query = input(\"You: \")\n",
        "\n",
        "    if query.lower() == 'exit':\n",
        "        print(\"Exiting the chatbot. Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Get the response from the QA chain\n",
        "    response = qa_chain.invoke({\"query\": query})\n",
        "\n",
        "    # Print the result\n",
        "    print(\"Bot: \", response[\"result\"])\n",
        "\n",
        "    # Optionally print the source document (first result)\n",
        "    if response.get(\"source_documents\"):\n",
        "        print(\"Source document: \", response[\"source_documents\"][0].metadata[\"source\"])\n",
        "\n",
        "    # Debug: Show current conversation memory (optional)\n",
        "    print(\"\\nMemory so far:\\n\", memory.buffer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "5ntVjmiKRIpm",
        "outputId": "b3f611d2-ec50-4a96-ee76-927078905568"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/models/WordEmbeddings.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  weights = torch.load(os.path.join(input_path, \"pytorch_model.bin\"), map_location=torch.device(\"cpu\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading vector database from: /content/content/auto_book_1_embed_full\n",
            "Loading vector database from: /content/content/auto_2000_new_embed_full\n",
            "Loading vector database from: /content/content/auto_2001_new_embed_full\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'combined_results' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-ea594e4789da>\u001b[0m in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mchain_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"stuff\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mretriever\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcombined_results\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mreturn_source_documents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mmemory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory\u001b[0m  \u001b[0;31m# Pass the memory here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'combined_results' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "UHxejDY5dMOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import (\n",
        "    create_history_aware_retriever,\n",
        "    create_retrieval_chain,\n",
        ")\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "# Add get_chat_history function\n",
        "\n",
        "def get_chat_history(memory: ConversationBufferMemory) -> str:\n",
        "    \"\"\"Extract and format chat history from memory.\"\"\"\n",
        "    if memory and memory.chat_memory.messages:\n",
        "        return \"\\n\".join([\n",
        "            f\"{msg.type.capitalize()}: {msg.content}\"\n",
        "            for msg in memory.chat_memory.messages\n",
        "        ])\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "template = \"\"\"Answer the question in your own words as truthfully as possible from the context given to you.\n",
        "If you do not know the answer to the question, simply respond with \"I don't know. Can you ask another question\".\n",
        "If questions are asked where there is no relevant context available, simply respond with \"I don't know. Please ask a question relevant to the documents\"\n",
        "Context: {context}\n",
        "\n",
        "\n",
        "{chat_history}\n",
        "Human: {question}\n",
        "Assistant:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"chat_history\", \"question\"], template=template\n",
        ")\n",
        "\n",
        "# Create the custom chain\n",
        "if llm is not None and vectordb is not None:\n",
        "    chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm, retriever=vectordb.as_retriever(), memory=memory,\n",
        "        get_chat_history=get_chat_history, return_source_documents=True,\n",
        "        combine_docs_chain_kwargs={'prompt': prompt})\n",
        "else:\n",
        "    logger.error(\"LLM or Vector Database not initialized\")\n",
        "\n",
        "# Chat on terminal\n",
        "print(\"Welcome to the AI Chat! Type 'exit' to end the chat.\")\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Pass the user input to the chain\n",
        "        response = chain({\"question\": user_input})\n",
        "        print(f\"Assistant: {response['answer']}\")\n",
        "\n",
        "        # Optional: Print source documents if needed\n",
        "        if 'source_documents' in response:\n",
        "            print(\"\\n--- Sources ---\")\n",
        "            for doc in response['source_documents']:\n",
        "                print(f\"{doc.metadata['source']}: {doc.page_content[:200]}...\")  # Adjust content display as needed\n",
        "            print(\"----------------\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ucpEKNwXPKu",
        "outputId": "6a1d4380-e3c6-4f68-f561-112a8fb3eed0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the AI Chat! Type 'exit' to end the chat.\n",
            "You: 2000 toyota rav 4 brakes\n",
            "An error occurred: 'list' object has no attribute 'chat_memory'\n",
            "You: quit\n",
            "Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# Initialize memory\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "\n",
        "# Ensure this code works with your chain definition\n",
        "if llm is not None and vectordb is not None:\n",
        "    chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=vectordb.as_retriever(),\n",
        "        memory=memory,\n",
        "        get_chat_history=get_chat_history,\n",
        "        return_source_documents=True,\n",
        "        combine_docs_chain_kwargs={'prompt': prompt}\n",
        "    )\n",
        "else:\n",
        "    logger.error(\"LLM or Vector Database not initialized\")"
      ],
      "metadata": {
        "id": "YZ_7OLxHr51_"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMOaoV9iN3Jl"
      },
      "source": [
        "# Practice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opf_cDN-jSiS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ab21258-885a-4a78-db4d-6039d7127eda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.11)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.7-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.2.9-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.10/dist-packages (0.1.143)\n",
            "Collecting langsmith\n",
            "  Downloading langsmith-0.1.144-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain_experimental\n",
            "  Downloading langchain_experimental-0.3.3-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.2.53-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.10/dist-packages (0.2.43)\n",
            "Collecting langchain_core\n",
            "  Downloading langchain_core-0.3.19-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.2)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.2-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.54.0 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (1.54.4)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith) (3.10.11)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Collecting langchain-community<0.4.0,>=0.3.0 (from langchain_experimental)\n",
            "  Downloading langchain_community-0.3.7-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.4 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.5-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.32 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.36-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
            "  Downloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.6.7)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community<0.4.0,>=0.3.0->langchain_experimental)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community<0.4.0,>=0.3.0->langchain_experimental)\n",
            "  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.4->langgraph) (1.1.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain_openai) (0.7.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain_openai) (4.66.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.9.11)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith) (1.2.2)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.23.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.9.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.0.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.0.0)\n",
            "Downloading langchain-0.3.7-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.2.9-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.1.144-py3-none-any.whl (310 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.1/310.1 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_experimental-0.3.3-py3-none-any.whl (208 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.0/209.0 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph-0.2.53-py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.1/125.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.19-py3-none-any.whl (409 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.3/409.3 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.7-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.2-py3-none-any.whl (25 kB)\n",
            "Downloading langgraph_checkpoint-2.0.5-py3-none-any.whl (24 kB)\n",
            "Downloading langgraph_sdk-0.1.36-py3-none-any.whl (29 kB)\n",
            "Downloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: SQLAlchemy, httpx-sse, tiktoken, pandas, matplotlib, pydantic-settings, langsmith, langgraph-sdk, langchain_core, langgraph-checkpoint, langchain-text-splitters, langchain_openai, langgraph, langchain, langchain-community, langchain_experimental\n",
            "  Attempting uninstall: SQLAlchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.36\n",
            "    Uninstalling SQLAlchemy-2.0.36:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.36\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.8.0\n",
            "    Uninstalling matplotlib-3.8.0:\n",
            "      Successfully uninstalled matplotlib-3.8.0\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.1.143\n",
            "    Uninstalling langsmith-0.1.143:\n",
            "      Successfully uninstalled langsmith-0.1.143\n",
            "  Attempting uninstall: langchain_core\n",
            "    Found existing installation: langchain-core 0.2.43\n",
            "    Uninstalling langchain-core-0.2.43:\n",
            "      Successfully uninstalled langchain-core-0.2.43\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.2.2\n",
            "    Uninstalling langchain-text-splitters-0.2.2:\n",
            "      Successfully uninstalled langchain-text-splitters-0.2.2\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.2.11\n",
            "    Uninstalling langchain-0.2.11:\n",
            "      Successfully uninstalled langchain-0.2.11\n",
            "  Attempting uninstall: langchain-community\n",
            "    Found existing installation: langchain-community 0.2.10\n",
            "    Uninstalling langchain-community-0.2.10:\n",
            "      Successfully uninstalled langchain-community-0.2.10\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "langchain-chroma 0.1.2 requires langchain-core<0.3,>=0.1.40, but you have langchain-core 0.3.19 which is incompatible.\n",
            "langchain-groq 0.1.6 requires langchain-core<0.3,>=0.2.2, but you have langchain-core 0.3.19 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed SQLAlchemy-2.0.35 httpx-sse-0.4.0 langchain-0.3.7 langchain-community-0.3.7 langchain-text-splitters-0.3.2 langchain_core-0.3.19 langchain_experimental-0.3.3 langchain_openai-0.2.9 langgraph-0.2.53 langgraph-checkpoint-2.0.5 langgraph-sdk-0.1.36 langsmith-0.1.144 matplotlib-3.9.2 pandas-2.2.3 pydantic-settings-2.6.1 tiktoken-0.8.0\n"
          ]
        }
      ],
      "source": [
        "%pip install -U langchain langchain_openai langsmith pandas langchain_experimental matplotlib langgraph langchain_core"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gh3SlKbx2p1w",
        "outputId": "86adb459-1917-40a7-f2db-5e13d8b417e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YdjP_67p3K0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ic3sPTtzg-7q"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain-groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(model=\"llama3-8b-8192\") #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"
      ],
      "metadata": {
        "id": "To1RKSxEWxVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qnouvhOC2j6P"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}